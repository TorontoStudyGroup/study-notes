---
sidebar_position: 9
slug: "/system-design-interview/ch9"
description: "Chapter 9: Design a Web Crawler"
last_update:
  date: 2025-09-06
  author: "Philly Chien"
---

import InterviewQuestion from '@site/src/components/InterviewQuestion';

# Chapter 9: Design a Web Crawler

:::note

What is a web crawler?

A web crawler (also known as a robot or spider) is used by search engines to discover new or updated content on the web.

The basic algorithm is:

1. Given a set of URLs, download the web pages.

2. Extract URLs from these pages.

3. Add new URLs to the list to be downloaded and repeat.

Characteristics of web crawlers:

- Scalability: Must be extremely efficient using parallelization to handle the web's massive size.

- Robustness: Must handle web traps like bad HTML, unresponsive servers, and malicious links.

- Politeness: Should not make too many requests to a website within a short time interval.

- Extensibility: Should be flexible so minimal changes are needed to support new content types.

:::

## Establish Design Scope

This phase clarifies the system's requirements and constraints. The main points derived from the Q&A with the interviewer are:

**Key Requirements**

- Purpose: Search engine indexing.

- Scale: Crawl 1 billion web pages per month.

- Content Type: HTML only.

- Freshness: The crawler must consider newly added or edited web pages.

- Storage: Store crawled HTML pages for up to 5 years.

- Deduplication: Pages with duplicate content should be ignored.

**Back-of-the-envelope Estimation**

The main points derived from the Q&A with the interviewer are:

- QPS (Queries Per Second)

1,000,000,000 pages / 30 days / 24 hours / 3600 seconds ≈ 400 QPS 

Peak QPS is estimated at 2 * QPS = 800 QPS.

- Storage

1 billion pages * 500k/page = 500 TB per month. (*Assuming an average page size of 500k*)

Total storage for 5 years: 500 TB/month * 12 months * 5 years = 30 PB.

## High-Level Architecture & Workflow

### High-Level Architecture

This is a breakdown of the main building blocks of the web crawler.

![High-Level Architecture](./images/ch9/high-level-architecture.svg)

- **Seed URLs**: The initial set of URLs that the crawler uses as a starting point.

- **URL Frontier**: Stores the URLs that are yet to be downloaded. Acts as the manager for the crawler's "to-do list".

- **HTML Downloader**: Fetches the raw HTML content of a web page from the internet.

- **DNS Resolver**: Translates a website's hostname (e.g., en.wikipedia.org) into an IP address.

- **Content Parser**: Parses the downloaded HTML to extract its structure and ensure it's not malformed.

- **Content Seen?**: Checks if the exact same content has been seen before (from a different URL) to prevent storing duplicates. Usually works by comparing hashes.

- **Content Storage**: The storage system (e.g., a database or a distributed file system) that holds the content of downloaded pages.

- **URL Extractor**: Extracts all hyperlink URLs from a downloaded HTML page.

- **URL Filter**: Filters out unwanted or irrelevant links based on predefined rules (e.g., file extensions, blacklisted sites).

- **URL Seen?**: A data structure (like a Bloom filter or hash table) that tracks if a URL has already been processed or is currently in the frontier to prevent redundant work and infinite loops.

- **URL Storage**: Stores all URLs that have already been visited.

### Workflow

This is the step-by-step process showing how a URL is processed through the system.

![High-Level Architecture](./images/ch9/workflow.svg)

1. **Start**: Add seed URLs to the URL Frontier.

2. **Fetch**: The HTML Downloader gets a list of URLs from the URL Frontier.

3. **Download**: The downloader gets the IP address from the DNS Resolver and downloads the page.

4. **Parse**: The downloaded page is sent to the Content Parser.

5. **Check Content Duplicates**: The parsed content is checked by "Content Seen?".

6. **Discard or Process**: If the content is a duplicate, it's discarded. If it's new, it's sent to the URL Extractor.

7. **Extract Links**: The URL Extractor pulls all links from the page content.

8. **Filter Links**: The extracted links are passed to the URL Filter.

9. **Check URL Duplicates**: The filtered links are checked by "URL Seen?".

10. **Discard or Add**: If a URL has already been visited or is in the frontier, it's discarded.

11. **Add to Frontier**: If the URL is new, it's added to the URL Frontier for future crawling. The cycle repeats.

## Deep Dive into Key Components and Challenges

### DFS vs. BFS

You can think of the web as a massive, directed graph. Crawling is essentially a graph traversal problem.

- **DFS (Depth-First Search)**: Generally not a good choice. A crawler using DFS could easily get stuck in a very deep "rabbit hole" within a single website (e.g., domain.com/a/b/c/...).

- **BFS (Breadth-First Search)**: Commonly used by web crawlers. It explores the web layer by layer. The URL Frontier is essentially a large-scale, sophisticated implementation of a BFS queue.

![Crawler Traversal Strategy](./images/ch9/crawler-traversal.svg)

### The URL Frontier
 
![URL Frontier](./images/ch9/url-frontier.svg)

 A simple FIFO queue for BFS is not enough. A well-designed URL Frontier must handle:
 
 #### Politeness
 
 - Problem: A single queue may batch many URLs from the same host, causing bursty, impolite traffic.
 - Design (Back queues model):
   - Create multiple queues, one per host.
   - A "Queue Router" assigns each URL to its host-specific queue.
   - Worker threads consume from one host at a time with a configurable delay between requests.
 
 #### Priority
 
 - Problem: Not all pages are equally important; FIFO has no notion of importance.
 - Design (Front queues model):
   - A Prioritizer assigns a score (e.g., PageRank, traffic).
   - Maintain multiple queues (f1, f2, ..., fn) by priority level.
   - A "Queue Selector" favors higher-priority queues when pulling work.

 #### Freshness
 
 - Problem: Web pages change frequently; recrawling everything uniformly is too slow and expensive.
 - Design:
   - Re-crawl pages based on estimated update frequency.
   - Prioritize important pages to be re-crawled more often.
 
 #### Storage
 
 - Problem: The frontier can contain hundreds of millions of URLs; RAM is insufficient and disk I/O can bottleneck.
 - Design (Hybrid approach):
   - Keep the majority of URLs on disk.
   - Use fast in-memory buffers for enqueue/dequeue, flushing to disk periodically.

### The HTML Downloader

The HTML Downloader is responsible for the actual network requests to fetch web pages. Its design involves a balance between performance and respecting website rules.

#### Robots Exclusion Protocol (robots.txt)

- What it is: robots.txt is a standard file that websites use to communicate with crawlers. It specifies which parts of the site crawlers are allowed or disallowed to access.
- Rule: A polite crawler must check a site's robots.txt file before crawling it and strictly follow its rules.
- Optimization: Cache robots.txt per host and refresh it periodically to avoid fetching on every request.

#### Performance and Reliability

- Distributed crawl: Distribute crawl jobs across multiple servers; each handles a subset of the URL space (horizontal scaling).

![Distributed Crawl](./images/ch9/distributed-crawl.svg)

- Cache DNS resolver: Maintain a local DNS cache to avoid the 10–200ms lookup penalty on every request.
- Locality: Place crawl servers near target regions to reduce network latency and speed up downloads.
- Short timeout: Enforce a maximum wait time; if a server does not respond within the timeout, abandon the request and move on.

### Robustness

A large-scale distributed system must be designed to be resilient to failures.

#### Techniques

- Consistent hashing: Distribute load (e.g., URLs) evenly across downloader servers to simplify scale out/in without massive rebalancing.
- Checkpoint crawl state: Periodically persist system state (e.g., URL Frontier) so a disrupted crawl can resume from the last checkpoint.
- Graceful exception handling: Handle frequent errors (network failures, malformed HTML) without crashing the system.
- Data validation: Validate inputs and intermediate data to prevent propagation of malformed data.

### Extensibility

The system should be flexible enough to support new content types and features in the future without a complete redesign.

![Crawler Extensibility](./images/ch9/crawler-extensibility.svg)

#### Design Approach

- Pluggable modules: Add capabilities without redesign (e.g., a PNG Downloader, or a Web Monitor for copyright checks).

### Detect and Avoid Problematic Content

The web is messy. The crawler must be able to handle redundant, meaningless, or harmful content.

#### Problems

- Duplicate content: Nearly 30% of web content is duplicated.
- Spider traps: Servers can generate infinite paths (e.g., .../foo/bar/foo/bar/...).
- Low-value noise: Ads, code snippets, or spam provide little value.

#### Mitigations

- Hash/checksum-based deduplication to detect and discard duplicates.
- Set a maximum URL length to avoid traps; ignore URLs exceeding the limit.
- Filter and exclude low-value content where possible.

## Further Considerations

This final section covers advanced topics and other important aspects for building a production-ready web crawler.

#### Server-side Rendering (SSR)

- Problem: Many modern websites generate links dynamically with JavaScript or AJAX, which a simple HTML downloader cannot discover.
- Solution: Perform server-side or dynamic rendering before parsing to execute page scripts and obtain the final HTML with all generated links.

#### Filter Unwanted Pages

- Rationale: Storage and crawl budget are finite; avoid low-quality content.
- Approach: Use an anti-spam/quality component to identify and filter spam or low-value pages so resources focus on meaningful content.

#### Data Layer: Replication and Sharding

- Replication: Create database copies to improve availability and read scalability.
- Sharding: Partition data across databases to scale writes and storage.
- Goal: Increase availability, scalability, and reliability of the data layer.

#### Horizontal Scaling

- Run on hundreds or thousands of servers.
- Design crawling servers to be stateless to enable elastic scaling and simplified operations.

#### Availability, Consistency, and Reliability

- Core concerns for any large system; evaluate trade-offs and apply them across components.

#### Analytics

- Collect and analyze telemetry to monitor crawler health and performance.
- Use insights from analytics to tune system behavior and improve efficiency.


### Mid-Level Engineer Interview Questions

<InterviewQuestion
  level="mid"
  question="Why do web crawlers typically use Breadth-First Search (BFS) over Depth-First Search (DFS)?"
  hint="Think about the structure of the web as a graph. What happens if you follow one path as deep as possible versus exploring layer by layer? Consider the risk of getting &quot;stuck.&quot;"
  answer="The web can be viewed as a large graph of pages (nodes) connected by links (edges).\n\n**The Problem with DFS:**\n- DFS explores a path to its absolute depth before backtracking.\n- A single site can have virtually infinite deep paths (e.g., dynamic calendars).\n- A DFS-based crawler could get stuck on one site for a very long time, missing the rest of the web.\n\n**The Advantage of BFS:**\n- Explores layer by layer starting from seed URLs (layer 0), then layer 1, 2, ...\n- Ensures broad discovery across many sites and pages.\n- Prevents getting trapped in one corner of the web.\n- The URL Frontier is essentially a queue that facilitates BFS traversal."
/>

<InterviewQuestion
  level="mid"
  question="Can you explain the main responsibilities of a URL Frontier? How does it differ from a simple queue?"
  hint="A simple queue just follows FIFO (First-In-First-Out). What other important non-functional requirements, like politeness and importance, does a crawler need to manage?"
  answer="A URL Frontier goes beyond basic FIFO, providing:\n\n- **Politeness management:** Avoids overloading any single host using host-based queues and enforced delays.\n- **Priority management:** Multiple priority queues so important pages (e.g., homepages) are crawled earlier.\n- **Large-scale storage:** Hybrid disk+memory design to handle billions of URLs efficiently.\n- **Coordination:** In distributed crawls, coordinates agents to prevent duplicate work."
/>

<InterviewQuestion
  level="mid"
  question="If you were asked to design a 'polite' crawler, how would you implement it to avoid making requests to the same website too frequently?"
  hint="Think about how you would organize the URLs. Would a single, large queue work, or would you need a more granular structure to manage different websites?"
  answer="Control the request rate on a per-host basis:\n\n- **Host-based queues:** Maintain per-host queues; a &quot;Queue Router&quot; assigns URLs by hostname (e.g., one for wikipedia.org, another for google.com).\n- **Throttled workers:** Each worker pulls from one host's queue and enforces a delay (e.g., 1s) between requests to that host."
/>

<InterviewQuestion
  level="mid"
  question="Why can DNS resolution become a bottleneck for a web crawler? How would you optimize it?"
  hint="Consider the high volume of requests a crawler makes per second and the inherent latency of a network call. What can you do to avoid making the same external network call over and over again?"
  answer="**Bottleneck:** Crawlers process hundreds/thousands of URLs per second. DNS lookups (hostname → IP) take ~10–200ms; doing one per URL adds massive latency.\n\n**Optimization:** Implement a local DNS cache. Check cache first; on miss, resolve and store with a TTL so subsequent requests avoid the external lookup."
/>

<InterviewQuestion
  level="mid"
  question="We need to avoid crawling the same URL twice and also avoid storing identical page content that comes from different URLs. How would you handle these two different kinds of duplication?"
  hint="One type of duplication happens before downloading the content, and the other happens after. What data structures or techniques would be efficient for each step?"
  answer="Handled at two stages:\n\n- **Duplicate URL prevention (&quot;URL Seen?&quot;):** Before scheduling. Use a Bloom filter or distributed hash set to detect previously seen URLs and discard them.\n- **Duplicate content prevention (&quot;Content Seen?&quot;):** After download. Hash the content (e.g., SHA-256) and check against a store of seen hashes; discard duplicates."
/>

<InterviewQuestion
  level="mid"
  question="What is robots.txt? Why is it essential for a crawler to respect it?"
  hint="Think about the relationship between website owners and web crawlers. How do owners communicate rules to automated bots?"
  answer="**What it is:** A standard text file at the domain root (e.g., `https://example.com/robots.txt`) that specifies which parts of a site specific user-agents may access.\n\n**Why it's essential:** The primary mechanism for administrators to control crawler traffic. Respecting it is core to politeness and ethical crawling, avoids sensitive/unnecessary pages, conserves resources, and prevents IP bans."
/>

<InterviewQuestion
  level="mid"
  question="What should a Crawler Agent do when it tries to download a URL, but the target server is unresponsive for a long time?"
  hint="How do you prevent a single slow or dead server from blocking your entire crawling operation?"
  answer="Never wait indefinitely:\n\n- **Short timeouts:** Enforce strict per-request timeouts (e.g., 2–5s); abort on timeout.\n- **Retry strategy:** Report to the Frontier and reschedule with exponential backoff; after repeated failures, discard the URL."
/>

<InterviewQuestion
  level="mid"
  question="When designing the 'URL Seen?' component, what data structures would you consider? What are their respective pros and cons?"
  hint="You need to store and check for the existence of billions of URLs. What are the trade-offs between memory usage and 100% accuracy?"
  answer="**Hash set / distributed hash table:**\n- Pros: 100% accuracy, O(1) average lookups.\n- Cons: Extremely high memory usage at web scale.\n\n**Bloom filter:**\n- Pros: Very memory-efficient, fast lookups.\n- Cons: Possible false positives (no false negatives); acceptable for crawlers."
/>

<InterviewQuestion
  level="mid"
  question="Please describe the complete lifecycle of a URL, from the moment it is discovered until its page content is stored."
  hint="Walk through the high-level architecture diagram step-by-step. Start with link extraction and end with content storage."
  answer="1. **Discovery:** URL Extractor finds a link while parsing a downloaded page.\n2. **Filtering:** URL Filter applies rules (blacklists, invalid formats).\n3. **URL dedup:** Check &quot;URL Seen?&quot;; if seen, stop.\n4. **Scheduling:** Add to URL Frontier with proper host and priority.\n5. **Downloading:** Agent picks URL, resolves DNS (cache first), downloads via HTTP.\n6. **Parsing:** Content Parser processes the HTML.\n7. **Content dedup:** Hash content and check &quot;Content Seen?&quot;; discard if duplicate.\n8. **Storage:** Save new content to Content Storage (e.g., S3).\n9. **Extraction:** Send content to URL Extractor to discover new links."
/>

<InterviewQuestion
  level="mid"
  question="What is a spider trap? Please give an example and explain how to avoid it."
  hint="Think about ways a website could dynamically generate a seemingly infinite number of unique URLs. How can you put a hard limit on this behavior?"
  answer="**What it is:** A site pattern that causes crawlers to generate endless requests for low-value pages.\n\n**Example:** Infinite calendars (e.g., .../cal/2098/10 → .../cal/2098/11), or faceted navigation producing many unique URLs with identical content.\n\n**Avoidance:** Set hard limits such as maximum URL length or path depth. The URL Filter discards URLs that exceed thresholds (e.g., length > 200 or depth > 10)."
/>

### Senior Engineer Interview Questions

<InterviewQuestion
  level="senior"
  question="What are the pros and cons of designing page parsing as a separate microservice versus embedding the logic within the Crawler Agent? Which would you choose for a large-scale system and why?"
  hint="Think about resource utilization profiles (CPU vs. I/O), independent scalability, and fault isolation."
  answer="This evaluates architectural trade-offs.\n\n**Embedded in Crawler Agent**\n**Pros:** Simpler to start; lower latency between download and parse; straightforward initial deployment.\n**Cons:** Becomes a monolith. Downloading is I/O-bound while parsing is CPU-bound, forcing you to scale both together. A parsing bug or memory leak can crash the entire agent and halt downloads.\n\n**Separate Parser Microservice**\n**Pros:** Independently scale CPU-heavy parsers and I/O-heavy agents; better fault isolation; technology flexibility per service.\n**Cons:** More operational complexity; small extra network hop.\n\n**Conclusion:** At large scale, a separate Parser service is superior because it lets you scale CPU vs. I/O independently for better cost efficiency, resilience, and maintainability."
/>

<InterviewQuestion
  level="senior"
  question="Our current system handles 1 billion pages/month. The new goal is 100 billion/month. What new bottlenecks would you anticipate, and how would you evolve the architecture for 100x scale?"
  hint="Beyond adding servers, consider central components that become single points of failure, and how data volume changes the problem."
  answer="100x scale introduces qualitative changes.\n\n**Anticipated bottlenecks:**\n- **URL Frontier:** A single service cannot manage state/storage/throughput for trillions of URLs.\n- **&quot;URL Seen?&quot; / Dedupe:** A single Bloom filter or hash set becomes infeasible; memory balloons and false-positive rates rise.\n- **Network egress:** Bandwidth and cost can dominate; single-DC saturation risk.\n\n**Architecture evolution:**\n- **Federated/Sharded Frontier:** Shard by hostname hash or TLD; each shard manages its own queues/storage.\n- **Sharded dedupe:** Use the same sharding key as the Frontier for data locality.\n- **Geo-distribution:** Deploy regional clusters (US/EU/APAC) to reduce latency and optimize bandwidth costs."
/>

<InterviewQuestion
  level="senior"
  question="Discuss storage options for the URL Frontier. What are the trade-offs between in-memory, disk-based, or hybrid? How would you ensure data consistency under high concurrency?"
  hint="Evaluate persistence, throughput, and capacity. For consistency, think about atomic leasing of a URL."
  answer="**Trade-offs:**\n- **In-memory (e.g., Redis):** Extremely fast but volatile and expensive at scale.\n- **Disk-based (e.g., RDBMS):** Durable and cost-effective but too slow for high QPS.\n- **Hybrid (recommended):** Disk as source of truth (e.g., append-only log or LSM like RocksDB) with in-memory buffers for active queues; write-through/periodic flush.\n\n**Consistency under concurrency:**\n- Implement **visibility timeouts (leases):** Move URL to an invisible &quot;in-flight&quot; state for T seconds.\n- On success, agent ACKs to delete; on timeout, URL returns to pending. Ensures at-least-once processing similar to SQS."
/>

<InterviewQuestion
  level="senior"
  question="How do we ensure the freshness of crawled data? Design a strategy to decide when and how often to re-crawl a page."
  hint="A single recrawl frequency is inefficient. Prioritize and use change signals."
  answer="**Adaptive, priority-based strategy:**\n- **Prioritize by importance:** PageRank, traffic, sitemaps.\n- **Change history:** Track content hashes; estimate update frequency from change rate.\n- **Adaptive scheduling:** Maintain a recrawl priority queue; schedule URLs just before they are likely to change (e.g., Poisson model).\n- **Leverage sitemaps:** Use &lt;lastmod&gt;/&lt;changefreq&gt; as signals."
/>

<InterviewQuestion
  level="senior"
  question="If the master node of the URL Frontier or the entire service goes down, what is your disaster recovery plan to minimize data loss and downtime?"
  hint="Think about redundancy, replicated state, and eliminating single points of failure."
  answer="**High availability design:**\n- **Clustered Frontier with consensus (Raft/Paxos/ZooKeeper):** Leader election; state changes appended to a replicated log.\n- **Durable persistence:** Persist the replicated log to durable storage.\n- **Periodic snapshots:** Recover by loading the last snapshot and replaying the log.\n- **Client resilience:** Agents backoff/retry while Frontier recovers."
/>

<InterviewQuestion
  level="senior"
  question="Major sites deploy anti-crawling mechanisms. If our crawler is being blocked, what multi-faceted strategy would you develop?"
  hint="Consider IPs, request patterns, identification, headless browsers, and non-technical options."
  answer="**Layered approach:**\n- **Politeness first:** Strict robots.txt adherence and conservative rates.\n- **Distributed egress IPs:** Rotate across regions/clouds to avoid single-range blocks.\n- **Mimic human behavior:** Rotate user-agents; vary headers; add jitter; adapt delays to host responsiveness.\n- **Selective headless rendering:** Use Puppeteer/Playwright only for high-value targets.\n- **Partnerships/APIs:** Prefer official data access for critical sites."
/>

<InterviewQuestion
  level="senior"
  question="Estimate monthly operational costs (bandwidth, compute, storage) to crawl 1 billion pages. State assumptions and identify the largest expense."
  hint="Break down storage, bandwidth, and compute with assumed prices."
  answer="**Assumptions:**\n- Avg page size: 500 KB\n- Storage: ~$0.02/GB-month; Bandwidth: ~$0.08/GB; Compute: ~$150/server-month\n\n**Storage:** 1B * 500KB = 500TB/month; after 1 year ≈ 6PB → 6,000,000GB * $0.02 ≈ $120k/month.\n**Bandwidth:** 500TB → 500,000GB * $0.08 ≈ $40k/month.\n**Compute:** ~100 servers → ≈ $15k/month.\n\n**Total ≈ $175k/month** after year 1. Largest expenses: storage and bandwidth."
/>

<InterviewQuestion
  level="senior"
  question="What key metrics would you monitor to measure system health, performance, and crawl quality?"
  hint="Metrics across Frontier, Agents, and crawl quality; beyond CPU/memory."
  answer="**Frontier/Queue:** Queue sizes by host/priority; queue latency (time in frontier).\n**Agents:** Pages/sec per agent and global; download latency; error rates by HTTP/DNS/timeout.\n**Crawl quality:** New URLs discovered per page; URL/content duplicate rates.\n**System KPIs:** End-to-end URL latency; cost per million pages."
/>

<InterviewQuestion
  level="senior"
  question="If we need to start crawling PDFs and images, what design changes are required? How can the initial design make this easier?"
  hint="Keep the core workflow content-agnostic; make handlers pluggable."
  answer="**Design for extensibility:**\n- **Content-agnostic core:** Fetch resource, then dispatch by MIME type.\n- **Dispatcher:** Route by Content-Type to the appropriate handler.\n- **New modules:** Add a PDF extractor (parse text, links) and an image metadata extractor (EXIF, dimensions).\n- **Storage updates:** Store content-specific metadata and choose appropriate storage paths."
/>

<InterviewQuestion
  level="senior"
  question="A Crawler Agent leases a URL and crashes before completion. How do you ensure the URL is not lost and is eventually re-processed?"
  hint="Two-step processing with leases and ACK to guarantee at-least-once."
  answer="**Lease (visibility timeout) pattern:**\n- Leasing moves a URL from pending to invisible in-flight for T seconds.\n- On success, the agent sends an ACK to delete it permanently.\n- On crash/timeout, the URL returns to pending for another agent to pick up. Ensures at-least-once processing."
/>
